在 self-attention 中，计算速度比内存速度快得多，因此进程(操作)越来越多地受到内存(HBM)访问的瓶颈。
PagedAttention是vLLM的核心技术，它解决了LLM服务中内存的瓶颈问题。传统的注意力算法在自回归解码过程中，
需要将所有输入令牌的注意力键和值张量存储在GPU内存中，以生成下一个令牌。
这些缓存的键和值张量通常被称为KV缓存。
PagedAttention采用了虚拟内存和分页的经典思想，允许在非连续的内存空间中存储连续的键和值。
通过将每个序列的KV缓存划分为块，PagedAttention可以高效地进行注意力计算。
PagedAttention的内存利用效率接近最优，仅浪费不到4%的内存。
此外，PagedAttention还支持高效的内存共享，进一步减少了复杂采样算法的内存开销，提高了吞吐量。